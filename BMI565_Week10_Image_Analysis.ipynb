{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI565 - Bioinformatics Programming & Scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 - Image Analysis\n",
    "\n",
    "** * Thanks to Ryan Swan for these materials.**\n",
    "\n",
    "1. Introduction to Imaging\n",
    "2. OpenCV\n",
    "    * Opening Files\n",
    "    * Converting between color scales\n",
    "3. Clustering Image Data\n",
    "    * Importing and formatting data\n",
    "    * Performing clustering using scikit-learn\n",
    "4. Classification with Image Data\n",
    "    * Importing data and labels for classification\n",
    "    * Performing classification using SVM in scikit-learn\n",
    "5. Summary and References\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "1. Python 2.7\n",
    "2. `opencv (cv2)` module\n",
    "3. `scikit-learn` module\n",
    "4. `numpy` module\n",
    "5. `matplotlib` module\n",
    "6. Data Files\n",
    "    - `./images/logo.jpg`\n",
    "    - `./images/digits.png`\n",
    "\n",
    "OpenCV should be installable using the `conda` command below, while all other packages should be included in the current Anaconda distribution.\n",
    "\n",
    "    conda install opencv\n",
    "\n",
    "** Note: when you install `opencv` using `conda`, the package manager may downgrade numpy and a couple other packages due to \"dependency requirements\". This may cause problems, so after installing `opencv` I recommend that you re-install numpy with `conda install numpy` (this shouldn't affect `opencv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging with OpenCV\n",
    "\n",
    "In programming, there are many data types that we can manipulate and analyze. One of the most common of these data types is imaging data. Images are usually defined as a two-dimensional plot of intensity values. Each cell of the plot is a picture-element or \"pixel\". \n",
    "\n",
    "The method of defining a pixel varies depending on the imaging data. Black and white images can be thought of as a 2D array with values ranging from a minimum of 0, representing black, to a maximum value, representing white. The maximum value can vary based on how much fidelity is needed. Color images can be displayed a number of different ways, with three dimensions (RGB = Red, Green, Blue) or four dimensions (CMYK = cyan, magenta, yellow, black), and often include an extra value representing the luminance of the pixel. It is also possible that pixels may represent data that is not visible to the human eye, like X-ray, gamma ray, or infrared data.\n",
    "\n",
    "## OpenCV\n",
    "\n",
    "The OpenCV package in python provides a number of tools to manipulate imaging data. Primarily the tools provided are used to open and convert imaging formats so that they can be analyzed and processed. Images are stored as numpy ndarrays, which represent multidimensional arrays of fixed size.\n",
    "\n",
    "## Opening and Viewing Images\n",
    "\n",
    "Our first order of business is to open an image file and inspect it. The \"logo.jpg\" file contains the OHSU logo in the compressed JPEG format.\n",
    "\n",
    "[http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read the image (1 = color, 0 = grayscale, -1 = unchanged w/ alpha channel)\n",
    "img = cv2.imread('./images/logo.jpg', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the img object. We can see from this that it is an array of values, most of which are [255, 255, 255]. This corresponds to a white pixel, where red, green, and blue values are all at the maximum intensity of 255. This tells us that the image is in 8-bit format, which allows for 2^8 levels of intensity (0-255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 900, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Image size\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image data\n",
    "img[0:10,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've opened the image we are able to view it in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show the image (arg1 = window name, arg2 = image itself)\n",
    "cv2.imshow('image',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command constructs a call to a window to display the image. We can see that the image is primarily white pixels, with a smattering of other colors. We can call the destroyAllWindows method to close the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Close display window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering On Imaging Data\n",
    "\n",
    "Now that we have an image as a python data type we are free to perform any number of calculations on the image. One useful piece of information is what the most common colors in the image are. We can think about each pixel as a point in three dimensional space, where the colors red, blue, and green are mapped to spacial values. In this spacial context, we can use clustering to find the areas in the space of colors where most pixels are located.\n",
    "\n",
    "The first thing we need to do is to convert our image from a two-dimensional array into a one-dimensional list of pixels. For this analysis, we are only interested in the color data and not in the location of each pixel within the image.\n",
    "\n",
    "Reshape and shape are methods of the ndarray class. Shape gives the dimensions of an n-dimensional array as a tuple with n positions. Reshape takes a tuple with n positions and reorganizes the array to fit that specification.\n",
    "\n",
    "Here we take the first two dimensions of the array (representing the 2d location information) and flatten it into a column of values where each row contains a single pixel's color intensity information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## reshape the image to be a list of pixels\n",
    "img = img.reshape((img.shape[0] * img.shape[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559800, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255],\n",
       "       [255, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to import the KMeans method from scikit-learn. We define that we want a model with 5 clusters. This will return the five \"neighborhoods\" where most pixel values are centered. Theoretically this should fit our image well, since there are exactly five colors in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Do clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clt = KMeans(n_clusters = 5)\n",
    "clt.fit(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 254.89787952,  254.76324373,  254.72873724],\n",
       "       [ 199.24570551,   48.60009653,    3.05789544],\n",
       "       [   9.35204856,  238.80918058,   79.77314112],\n",
       "       [ 234.16859763,  168.33783496,  122.88442675],\n",
       "       [  33.98472538,  200.29493013,  251.93532662]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## View the cluster centers\n",
    "clt.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define two functions that will help us analyze and display the data. The first is a function to create a histogram that represents the number of pixels associated with each cluster center. The function uses numpy (imported above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def centroid_histogram(clt):\n",
    "    ## grab the number of different clusters and create a histogram\n",
    "    ## based on the number of pixels assigned to each cluster\n",
    "    numLabels = np.arange(0, len(np.unique(clt.labels_)) + 1)\n",
    "    (hist, _) = np.histogram(clt.labels_, bins = numLabels)\n",
    " \n",
    "    ## normalize the histogram, such that it sums to one\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= hist.sum()\n",
    " \n",
    "    ## return the histogram\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function uses the histogram to create a bar that displays the proportion of the image represented by each cluster centroid. In essence, we use the histogram percentage to populate a 50x300 pixel bar with color information. We use the rectangle method to reorganize our bar into the correct display format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_colors(hist, centroids):\n",
    "    ## initialize the bar chart representing the relative frequency\n",
    "    ## of each of the colors\n",
    "    bar = np.zeros((50, 300, 3), dtype = \"uint8\")\n",
    "    startX = 0\n",
    " \n",
    "    ## loop over the percentage of each cluster and the color of\n",
    "    ## each cluster\n",
    "    for (percent, color) in zip(hist, centroids):\n",
    "        ## plot the relative percentage of each cluster\n",
    "        endX = startX + (percent * 300)\n",
    "        cv2.rectangle(bar, (int(startX), 0), (int(endX), 50),\n",
    "            color.astype(\"uint8\").tolist(), -1)\n",
    "        startX = endX\n",
    "    \n",
    "    ## Convert back to RGB (implicit conversion takes place when clustering)\n",
    "    bar = cv2.cvtColor(bar, cv2.COLOR_BGR2RGB)\n",
    "    ## return the bar chart\n",
    "    return bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the necessary tools to display our color plot of the OHSU logo image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10da12110>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABNCAYAAACRzx5qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAb5JREFUeJzt2C1PXEEYQOG7pJCgMIup4bqKJghMK5v+M/4WTfBkA7If\niqBBNampmYoqEgKsWPZAnsfNZCbzqiNmMcaYANi+nW0PAMB/ggwQIcgAEYIMECHIABGCDBAhyAAR\nggwQIcgAEe/WObxcLsc8zxsaBXgJl9//bHuEV+Pgw68H9/d/Hz967/3ej3vrq59/78YYh0+9t1aQ\n53meVqvVOleAmN2Ti22P8Gp8Of/64P7Hs2+P3judP91b736+vnnOe74sACIEGSBCkAEiBBkgQpAB\nIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEi\nBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIE\nGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZ\nIGIxxnj+4cXidpqmm82NA/AmHY0xDp86tFaQAdgcXxYAEYIMECHIABGCDBAhyAARggwQIcgAEYIM\nECHIABH/ACTjJglNq6aVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9f2210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## build a histogram of clusters and then create a figure\n",
    "## representing the number of pixels labeled to each color\n",
    "hist = centroid_histogram(clt)\n",
    "bar = plot_colors(hist, clt.cluster_centers_)\n",
    " \n",
    "## show our color bart\n",
    "plt.figure()\n",
    "plt.axis(\"on\")\n",
    "## Remove the ticks and labels from the plot\n",
    "plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
    "plt.imshow(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114bcf790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We can also create a bar plot to represent the same info\n",
    "import matplotlib as mpl\n",
    "## Convert the colors from GBR to RGB and scale the values\n",
    "colors = [c[::-1]/255 for c in clt.cluster_centers_]\n",
    "plt.bar(np.arange(1,6), hist, width=0.9, color=colors)\n",
    "## Create x-axis ticks and labels (converting RGB colors to hexadecimal)\n",
    "ticks1 = plt.xticks(np.arange(1,6), [mpl.colors.rgb2hex(x) for x in colors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plots capture the five major colors used in the image and their relative frequencies. As we can see, the image is mostly white and blue with a sprinkling of other colors mixed in. If we wish to know the exact proportions it is possible to use the histogram object in order to find these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Digits from Imaging Data Using an SVM\n",
    "\n",
    "While clustering is a useful way of describing data, it is not always the best tool for classification. In cases where we have labeled data, a supervised learning method is often preferable. One canonical example is the recognition of digits from handwriting. \n",
    "\n",
    "As before, our first task is to open the file and get the image into a format that makes sense for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Open the image (remember the 0 denotes that this is a grayscale image)\n",
    "img2 = cv2.imread('./images/digits.png',0)\n",
    "\n",
    "cv2.imshow('image',img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## When you are done inspecting the image, close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the image in this case is one large image containing many examples of handwritten digits. In order for us to analyze the image we must first restructure the data so that it is a single column where each row contains the intensity values for a single handwritten digit. \n",
    "\n",
    "The ndarray contains two additional methods that allow us to perform this process. The hsplit method splits an image on the horizontal axis while the vsplit method splits an image along the vertical axis. Once we determine that the data is arranged with 100 columns and 50 rows, all evenly spaced, we can write a list comprehension that quickly performs this rearrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image size\n",
    "img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image data (it's gray-scale this time)\n",
    "img2[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This tells OpenCV that the image is arranged in 50 rows and 100 columns (5000 digits)\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(img2,50)]\n",
    "\n",
    "## Make an array from the data\n",
    "x = np.array(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 100, 20, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now each digit (N=5000) is a block of 20x20 pixels\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells object now contains a 50x100 array where each cell contains a handwritten digit (20x20 pixels). We will need to reshape the data further, but while it is in a convenient format, we can define our training and test sets by splitting the array in half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   9  33   9   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  41 177 249 178  29   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  33 198 255 240 255 107   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1  70 199 255 255 197 154 253  98   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0  45 238 255 205 224 222  83 224 128   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0  25 202 255 193  40  99  54   0 190 197  16   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0  20 163 246 152  72   0   0   0   0 184 252  74   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0  97 255 118   0   1   0   0   0   0 184 255  82   0\n",
      "    0   0]\n",
      " [  0   0   0   0  20 218 216  17   0   0   0   0   0   0 183 255  78   0\n",
      "    0   0]\n",
      " [  0   0   0   0  67 255 138   0   0   0   0   0   0  24 215 188  22   0\n",
      "    0   0]\n",
      " [  0   0   0   0  69 247  87   0   0   0   0   0  31 185 212  54   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  71 237  68   0   0   0   0  94 211 177  17   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  69 255 170  39  39 115 183 238 185  37   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  57 245 255 229 233 255 216 117  13   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   7 114 230 255 234 137  38   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0  26  38  27   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]]\n"
     ]
    }
   ],
   "source": [
    "## The first digit\n",
    "print x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now we prepare training data and test data\n",
    "## Takes the first 50 columns and puts them into the training set\n",
    "train = x[:,:50]\n",
    "\n",
    "## reshape takes these and makes them 2500 rows and 400 (20x20) columns\n",
    "## giving -1 as one of the dimensions will tell numpy to determine the value \n",
    "## based on the length of the array and the other dimensions given\n",
    "train = train.reshape(-1,400)\n",
    "\n",
    "## the astype function converts to the float type that the clustering method is expecting\n",
    "train = train.astype(np.float32) ## Size = (2500,400)\n",
    "\n",
    "## We do the same for the test set\n",
    "test = x[:,50:100]\n",
    "test = test.reshape(-1,400)\n",
    "test = test.astype(np.float32) ## Size = (2500,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our classifier, we need to have labeled data. From the structure of our data, we know that each number is in a group, so we can specify a list of values to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create labels for train and test data\n",
    "## We have numbers 0 to 10...\n",
    "k = np.arange(10)\n",
    "## In sets of 250\n",
    "## We create the labels as a 1D array\n",
    "train_labels = np.repeat(k,250)\n",
    "\n",
    "## our test labels are the same as our training labels, so we can copy this over\n",
    "test_labels = train_labels.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to move on to peforming our model fitting. We use a support vector machine (SVM) which considers each pixel intensity value in the digit as a dimension in euclidian space. The SVM then attempts to draw an optimized boundary between points of different classes in order to classify them into groups. Since we have 10 classes (numbers 0-9) it will divide them into ten categories.\n",
    "\n",
    "We also load the metrics method, which contains reports that work with scikit-learn's classifiers in order to provide information about the performance of the classifier. Here we use the `classification_report` and `confusion_matrix` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0):\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.93      0.92       250\n",
      "          1       0.94      0.93      0.93       250\n",
      "          2       0.83      0.80      0.82       250\n",
      "          3       0.73      0.78      0.75       250\n",
      "          4       0.87      0.78      0.82       250\n",
      "          5       0.80      0.80      0.80       250\n",
      "          6       0.90      0.92      0.91       250\n",
      "          7       0.85      0.88      0.86       250\n",
      "          8       0.76      0.72      0.74       250\n",
      "          9       0.73      0.78      0.75       250\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Import modules from scikit-learn\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "## Train an SVM classifier\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train, train_labels)\n",
    "\n",
    "## Predict the classes of the test set\n",
    "predicted = classifier.predict(test)\n",
    "print(\"Classification report for classifier %s:\\n\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(test_labels, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[232   0   5   6   0   0   4   0   3   0]\n",
      " [  0 232   1   4   0   1   0   1  11   0]\n",
      " [  5   4 201  10   5   4   5   6   8   2]\n",
      " [  2   1   9 195   1  18   1   4   6  13]\n",
      " [  2   2   1   5 195   3   7   3   8  24]\n",
      " [  3   1   3  19   3 200   2   0   9  10]\n",
      " [  2   1   8   1   0   5 229   0   3   1]\n",
      " [  0   1   3   5   3   2   0 219   1  16]\n",
      " [  1   6  10  17   6  15   5   3 180   7]\n",
      " [  5   0   2   5  11   2   2  21   8 194]]\n"
     ]
    }
   ],
   "source": [
    "## View a confusion matrix of the classification results\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that our classifier (sort of) worked. We get good precision, showing that our method does a decent job of minimizing false positives. We can also see that our recall is relatively high, meaning that it calls fairly few false negatives. From our graph it seems the numerals 3, 8, and 9 are the hardest to predict, while 0, 1, and 6 are the easiest.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This lecture gives two specific examples of ways in which imaging data can be manipulated to draw interesting conclusions. The field of imaging is much larger than presented here, however, and python has many packages that perform various imaging tasks. Some common tasks include image enhancement, image filtering, and image segmentation (the isolation of various features of interest based on their attributes). While the packages necessary to do such manipulations may be different, the underlying workflow remains essentially the same (reading the image data, reformatting the image data, and using that data in order to find the features of interest).\n",
    "\n",
    "## In-class Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find an image of your own and perform the clustering process on it. \n",
    "## Try modifying the number of clusters used. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The following function normalizes imaging data based on the image's \n",
    "## second order moments. Use this function to process the training and\n",
    "## test sets of digits then rerun the analysis. Does this improve the\n",
    "## classifier's performance?\n",
    "##\n",
    "## Deskew Explanation: http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv.html#svm-opencv\n",
    "affine_flags = cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR\n",
    "\n",
    "def deskew(img, SZ=20): ## SZ is the size of each digit (20x20)\n",
    "    m = cv2.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    img = cv2.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "- [OpenCV Tutorial](http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_image_display/py_image_display.html)\n",
    "- [Image Clustering Tutorial](http://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/)\n",
    "- [OpenCV Digit Classification Tutorial](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Updated: 23-Sep-2016"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
